---
layout: post
name: VGG16 AND VGG19
title: VGG16 AND VGG19
categories: 
- ml
---

docs link: [https://keras.io/api/applications/vgg/](https://keras.io/api/applications/vgg/)<br/><br/>

## What is a Pre-trained Model? 
<br/>

A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset. <br/><br/>

VGG19 and VGG16 are also examples of pre-trained models on a large dataset of images, belonging to 1000 different classes, called ImageNet.<br/><br/>


## Why use a Pre-trained Model?  
<br/>
Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.<br/><br/>

## ABOUT VGG19 AND VGG16
<br/>
Very Deep Convolutional Networks for Large-Scale Image Recognition In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and second places in the localization and classification tracks respectively. We also show that our representations generalize well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.
<br/><br/>

VGG16 is a convolution neural net (CNN ) architecture that was used to win ILSVR(Imagenet) competition in 2014. It is considered to be one of the excellent vision model architectures till date. 
<br/><br/>
In the end, it has 2 FC(fully connected layers) followed by a softmax for output.<br/><br/>

The 16 in VGG16 refers to it has 16 layers that have weights. This network is a pretty large network and it has about 138 million (approx) parameters.<br/><br/>

